{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf98efe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- D√âFINITION DES CHEMINS (Identiques au script clustering.py) ---\n",
    "PROJECT_ROOT_ABSOLUTE = \"/home/onyxia/work/Gestion-portefeuille/\" \n",
    "\n",
    "try:\n",
    "    ROOT_DIR = Path(PROJECT_ROOT_ABSOLUTE)\n",
    "except Exception:\n",
    "    ROOT_DIR = Path.cwd() \n",
    "\n",
    "INTERIM_DATA_PATH = ROOT_DIR / \"data\" / \"interim\"\n",
    "PROCESSED_DATA_PATH = ROOT_DIR / \"data\" / \"processed\"\n",
    "INPUT_FILENAME = \"cac40_interim_features.csv\"\n",
    "\n",
    "\n",
    "# --- 1. FONCTION DE CHARGEMENT ET PR√âPARATION (Copie de la logique du script) ---\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\" Charge les donn√©es, s√©lectionne la derni√®re date et met √† l'√©chelle. \"\"\"\n",
    "    filepath = INTERIM_DATA_PATH / INPUT_FILENAME\n",
    "    if not filepath.exists():\n",
    "        print(f\"‚ùå Erreur : Fichier d'entr√©e non trouv√© √† {filepath}. Relancez l'√©tape 2.\")\n",
    "        return None, None\n",
    "\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    latest_date = df['Date'].max()\n",
    "    df_latest = df[df['Date'] == latest_date].copy().set_index('Ticker')\n",
    "    \n",
    "    FEATURES = ['Volatility', 'Sharpe_Ratio_20D', 'Performance_20D', 'Volume', 'Dividends']\n",
    "    \n",
    "    X = df_latest[FEATURES].copy()\n",
    "    \n",
    "    # Nettoyage et Imputation\n",
    "    X = X.replace([np.inf, -np.inf], np.nan)\n",
    "    X = X.fillna(X.mean())\n",
    "\n",
    "    # Normalisation\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=FEATURES, index=X.index)\n",
    "    \n",
    "    return X_scaled, df_latest\n",
    "\n",
    "\n",
    "# --- Ex√©cution de l'exploration ---\n",
    "X_scaled, df_latest = load_and_prepare_data()\n",
    "\n",
    "if X_scaled is None:\n",
    "    print(\"Impossible de continuer sans donn√©es pr√©par√©es.\")\n",
    "else:\n",
    "    # ----------------------------------------------------\n",
    "    # I. DIAGNOSTIC K (M√©thode du Coude)\n",
    "    # ----------------------------------------------------\n",
    "    print(\"\\nüî¨ Diagnostic : M√©thode du Coude pour K Optimal\")\n",
    "    \n",
    "    sse = {} # Sum of Squared Errors\n",
    "    for k in range(1, 10):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        sse[k] = kmeans.inertia_\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(list(sse.keys()), list(sse.values()), marker='o')\n",
    "    plt.title('M√©thode du Coude pour K Optimal')\n",
    "    plt.xlabel(\"Nombre de Clusters (K)\")\n",
    "    plt.ylabel(\"Somme des Carr√©s des Erreurs (SSE)\")\n",
    "    \n",
    "    # Sauvegarde du plot\n",
    "    plt.savefig(PROCESSED_DATA_PATH / \"diagnostic_elbow_method.png\")\n",
    "    plt.show() \n",
    "\n",
    "    # ----------------------------------------------------\n",
    "    # II. VISUALISATION DES CLUSTERS (PCA)\n",
    "    # ----------------------------------------------------\n",
    "\n",
    "    # R√©applique K-Means avec K=4 (valeur choisie dans votre script principal)\n",
    "    K_CHOICE = 4\n",
    "    kmeans_final = KMeans(n_clusters=K_CHOICE, random_state=42, n_init=10)\n",
    "    clusters = kmeans_final.fit_predict(X_scaled)\n",
    "    df_latest['Cluster'] = clusters\n",
    "    \n",
    "    print(f\"\\nüìä Visualisation de la s√©paration pour K = {K_CHOICE}\")\n",
    "    \n",
    "    # R√©duction de Dimension (PCA pour 2D)\n",
    "    pca = PCA(n_components=2)\n",
    "    principal_components = pca.fit_transform(X_scaled)\n",
    "    pca_df = pd.DataFrame(data = principal_components, columns = ['PC1', 'PC2'], index=df_latest.index)\n",
    "\n",
    "    # Ajouter la colonne Cluster pour la visualisation\n",
    "    pca_df['Cluster'] = clusters\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(pca_df['PC1'], pca_df['PC2'], \n",
    "                          c=pca_df['Cluster'], \n",
    "                          cmap='viridis', \n",
    "                          s=50,\n",
    "                          alpha=0.7) \n",
    "                          \n",
    "    # Annoter les points avec le ticker (pour identification)\n",
    "    for i, txt in enumerate(pca_df.index):\n",
    "        plt.annotate(txt, (pca_df['PC1'][i], pca_df['PC2'][i]), fontsize=8, alpha=0.8)\n",
    "\n",
    "    plt.title(f'Segmentation des Actifs du CAC 40 (K={K_CHOICE} - Visualisation PCA)')\n",
    "    plt.xlabel(f\"Composante Principale 1\")\n",
    "    plt.ylabel(f\"Composante Principale 2\")\n",
    "    \n",
    "    # L√©gende pour la couleur\n",
    "    legend1 = plt.legend(*scatter.legend_elements(),\n",
    "                         loc=\"upper right\", title=\"Clusters\")\n",
    "    plt.gca().add_artist(legend1)\n",
    "    \n",
    "    # Sauvegarde du plot\n",
    "    plt.savefig(PROCESSED_DATA_PATH / \"visualization_pca_clusters.png\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
